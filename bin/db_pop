#!/usr/bin/env python3

import sqlite3
import re
import os 
import json
import sys
from math import nan
import pandas as pd

#
# handle command line arguments
#
project = ""
server_dir = "./server"
if len(sys.argv) < 2:
    print("ERROR: must include path to project directory")
    exit(1)
project = sys.argv[1]

if len(sys.argv) > 2:
    server_dir= sys.argv[2]

#
# establish the generated data directory and files 
#
db_generated = os.path.join(project, "generated")
if not os.path.isdir(db_generated):
    os.mkdir(db_generated)

db = os.path.join(db_generated, 'generated-project.db')
if os.path.isfile(db):
    print("Removing existing database ...")
    os.remove(db)

#
# create the database connection
#
print("Populating database: {}".format(db))
conn = sqlite3.connect(db)
c = conn.cursor()

#
# read the project file
#
jsondata = {} 
with open(os.path.join(project, "project.json")) as jdata:
    jsondata = json.load(jdata)

# ---------------------------------------------------------------------------
# GTK table
# ---------------------------------------------------------------------------
# get the current version
gtkversion = None
with open(os.path.join(server_dir, "version.md"), "r") as vfile:
    gtkversion = vfile.readline().strip()

    # start from scratch
c.execute('''DROP TABLE IF EXISTS gtk''')
    # create the table 
c.execute('''CREATE TABLE gtk (version TEXT)''')
c.execute('''INSERT INTO gtk (version) VALUES(?)''', [gtkversion])


# ---------------------------------------------------------------------------
# PROJECT table
# ---------------------------------------------------------------------------
    # start from scratch
c.execute('''DROP TABLE IF EXISTS project''')
    # create the table 
c.execute('''CREATE TABLE project (name TEXT, title TEXT, num_segments INT)''')
print("Creating project table ...")
print("")

# ---------------------------------------------------------------------------
# DATASETS table 
# ---------------------------------------------------------------------------
    # start from scratch
c.execute('''DROP TABLE IF EXISTS datasets''')
    # create the table 
c.execute('''CREATE TABLE datasets (ID TEXT, name TEXT, epigenetics TEXT, structure TEXT)''')
insert = '''INSERT INTO datasets (ID,name,epigenetics,structure) values(?,?,?,?)'''
print("Creating datasets table ...")
for d in jsondata["datasets"]:
    c.execute(insert, [d["id"], d["name"], d["epigenetics"], d["structure"]["id"]]) 
print("")

# ---------------------------------------------------------------------------
# STRUCTURE table
# ---------------------------------------------------------------------------
    # start from scratch
c.execute('''DROP TABLE IF EXISTS structure''')
c.execute('''DROP TABLE IF EXISTS structure_metadata''')
    # create the table 
c.execute('''CREATE TABLE structure (structureid INT, segid INT, startid INT, endid INT, length INT, startx REAL, starty REAL, startz REAL, endx REAL, endy REAL, endz REAL, centerx REAL, centery REAL, centerz REAL)''')
c.execute('''CREATE TABLE structure_metadata (id INT, num_segments INT, interval INT, unmapped TEXT)''')

# parse all structure files, and insert data into the table
# a segment is based on two points from the file - the 
# current point, and the previous one. This takes advantage 
# of the fact that the points are contiguous in the sequence
print("Creating structure table ...")
for geom in jsondata["data"]["structure"]:
    infile = os.path.join( project, geom["url"])
    insert = '''INSERT INTO structure (structureid,segid,startid,endid,length,startx,starty,startz,endx,endy,endz,centerx,centery,centerz) VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?)'''
    meta_insert = '''INSERT INTO structure_metadata (id,num_segments,interval,unmapped) VALUES(?,?,?,?)'''
    print("    reading: {}".format(infile))
    position = 0 
        # the first position is arbitrary, and will be re-computed after all values are inserted 
    prev = [0.0, 0.0, 0.0]

    # create metadata entry
    c.execute(meta_insert, [geom["id"], geom["num_segments"], geom["interval"], json.dumps(geom["unmapped_segments"])] )

    # create structure data entries
    if (infile.endswith('.pdb')) :
        print("    loading structure file (PDB)")
        with open(infile, 'r') as indata:
            for l in indata:
                if l.startswith("ATOM"): 
                        # start at position 26, based on the format of the file, to 
                        # guarantee that you get the values (sometimes there is no
                        # whitespace that you can split on)
                    v      = re.split(r'\s+', l[26:]) 
                        # get the first few values
                    aid    = re.split(r'\s+', l)
                    cur    = [float(v[1]), float(v[2]), float(v[3])]
                    center = [ (prev[0]+cur[0])/2.0, (prev[1]+cur[1])/2.0, (prev[2]+cur[2])/2.0 ]
                    c.execute(insert, [geom["id"], aid[1], position, position + geom["interval"], geom["interval"], 
                        prev[0], prev[1], prev[2], cur[0], cur[1], cur[2], center[0], center[1], center[2]])
                    prev = cur
                    position = position + geom["interval"]

    elif (infile.endswith('.csv')) :
        print("    loading structure file (CSV)")
        with open(infile, 'r') as indata:
            df = pd.read_csv(infile)
            for e in df.to_dict(orient='records'):
                cur    = [float(e['x']), float(e['y']), float(e['z'])]
                center = [ (prev[0]+cur[0])/2.0, (prev[1]+cur[1])/2.0, (prev[2]+cur[2])/2.0 ]
                c.execute(insert, [geom["id"], e['id'], position, position + geom["interval"], geom["interval"], 
                    prev[0], prev[1], prev[2], cur[0], cur[1], cur[2], center[0], center[1], center[2]])
                prev = cur
                position = position + geom["interval"]

    else :
        print("ERROR: unknown structure file type: {}".format(infile))


    # compute an approximation for the endpoint of the first segment, 
    # based on the data this reflects the endpoint from the second 
    # segment around the endpoint of the first segment
        # first
    query  = conn.execute("SELECT * FROM structure where segid == 1 AND structureid == {}".format(geom["id"]))
    first  = query.fetchone()
        # second point
    query  = conn.execute("SELECT * FROM structure where segid == 2 AND structureid == {}".format(geom["id"]))
    point = query.fetchone()
        # compute new values and insert
    new    = [ (2.0*point[5] - point[8]), (2.0*point[6] - point[9]), (2.0*point[7] - point[10]) ]
    center = [ (new[0]+first[8])/2.0, (new[1]+first[9])/2.0, (new[2]+first[10])/2.0 ]
    c.execute("update structure set startx = {}, starty = {}, startz = {}, centerx = {}, centery = {}, centerz = {} where segid == 1 AND structureid == {}".format(new[0], new[1], new[2], center[0], center[1], center[2], geom["id"]))

print("")

# ---------------------------------------------------------------------------
# end of STRUCTURE table 
# ---------------------------------------------------------------------------

# ---------------------------------------------------------------------------
# ARRAY table
# ---------------------------------------------------------------------------
    # start from scratch
c.execute('''DROP TABLE IF EXISTS array''')
    # create the table 
c.execute('''CREATE TABLE array (id INT, name TEXT, type TEXT, min REAL, max REAL, url TEXT)''')

print("Creating array table")
if "array" in jsondata["data"]: 
    for a in jsondata["data"]["array"]:
        print("    loading {}".format(a["url"]))
        arraydata = {}
        with open(os.path.join(project, a["url"]), 'r') as afile:
            adata = json.load(afile)
        c.execute('''INSERT INTO array (id,name,type,min,max,url) VALUES (?,?,?,?,?,?)''', 
                    [a["id"], adata["name"], adata["type"], adata['data']['min'], adata['data']['max'], a["url"]])

print("")

# ---------------------------------------------------------------------------
# end of ARRAY table
# ---------------------------------------------------------------------------

# ---------------------------------------------------------------------------
# CONTACTS (Hi-C) table
# ---------------------------------------------------------------------------

    # start from scratch
c.execute('''DROP TABLE IF EXISTS contactmap''')
    # create the table 
c.execute('''CREATE TABLE contactmap (id INT, x INT, y INT, value REAL)''')

print("Creating contactmap records table")
for contact_map in jsondata["data"]["md-contact-map"]:
    map_id = contact_map["id"]
    infile = os.path.join( project, contact_map["url"] )
    insert = '''INSERT INTO contactmap (id,x,y,value) VALUES (?,?,?,?)'''

    print("    reading: {}".format(infile))
    with open(infile, 'r') as indata:
        for l in indata:
            columns = l.split("\t")
            try:
                columns[2] = float(columns[2])
            except ValueError:
                columns[2] = nan
            c.execute(insert, [map_id, *columns])
print("")

# ---------------------------------------------------------------------------
# end of CONTACTS table
# ---------------------------------------------------------------------------

# ---------------------------------------------------------------------------
# GENES table
# ---------------------------------------------------------------------------
    # start from scratch
c.execute('''DROP TABLE IF EXISTS genes''')
    # create the table
c.execute('''CREATE TABLE genes (id INTEGER PRIMARY KEY AUTOINCREMENT, start INT, end INT, length INT, gID TEXT, gene_type TEXT, gene_name TEXT)''')

# populate the table from a file
insert = '''INSERT INTO genes (start,end,length,gID,gene_type,gene_name) VALUES(?,?,?,?,?,?)'''
print("Creating genes table ...")

# check if there is an annotations file in the project
if "annotations" in jsondata["data"]:
    infile = os.path.join( project, jsondata["data"]["annotations"]["url"])
    with open(infile, 'r') as indata:
        print("    reading: {}".format(infile))
        alltypes = []
        noname_types = []
        for l in indata:
            data = l.strip()
            v = re.split(r'\t+', data) 

            # TODO: check the version of the file
            #       this works on version 3 of the format

            # check the type
            # new behavior (currently breaks all tests, to be updated ...)
            new_excluded_types = [
                                'biological_region',
                                'CDS',
                                'chromosome',
                                'exon',
                                'five_prime_UTR',
                                'pseudogene',
                                'pseudogenic_transcript',
                                'three_prime_UTR',
                                'unconfirmed_transcript'
                             ]
            # old behavior (only 'gene' is not excluded)
            # for release: removed ncRNA_gene
            excluded_types = [ 
                                'biological_region', 
                                'CDS', 
                                'chromosome', 
                                'exon', 
                                'five_prime_UTR', 
                                'lnc_RNA', 
                                'miRNA', 
                                'mRNA', 
                                'ncRNA', 
                                'pseudogene', 
                                'pseudogenic_transcript', 
                                'rRNA', 
                                'scRNA', 
                                'snoRNA', 
                                'snRNA', 
                                'three_prime_UTR', 
                                'unconfirmed_transcript'
                            ]

            if (len(v) > 1):

                # if you need to keep track of the types, you can print this out
                if v[2] not in alltypes:
                    alltypes.append(v[2])

                if (v[2] not in excluded_types): 
                    pairs = v[8].split(";")
                    values = {}
                    for p in pairs:
                        (key, value) = p.split("=")
                        values[key] = value

                    # check for possible errors
                    # this is obsolete, but is left in as a reminder that checks on data
                    # should be integrated into this process
                    if False:
                        if (int(v[3]) > limit):
                            print("ERROR in start: {}".format(v[3]))
                        if (int(v[4]) > limit):
                            print("ERROR in end:  {}".format(v[4]))

                    # insert the data into the table
                    length = int(v[4]) - int(v[3])

                    # get the name
                    name = "unknown"
                    if "Name" in values:
                        name = values["Name"]
                    elif "logic_name" in values:
                        name = values["logic_name"] 
                    else:
                        # collect the list of unnamed types
                        if v[2] not in noname_types:
                            noname_types.append(v[2])

                        # optionally print out anything without a name
                        # print("No name found: {}".format(l))

                    c.execute(insert, [v[3], v[4], length, values["ID"], values["biotype"], name]) 

# ---------------------------------------------------------
# if there is an annotations file, insert that data as well
# ---------------------------------------------------------
annotations_file = os.path.join( project, "source/annotations.csv" ) 
if os.path.isfile(annotations_file):
    print("    processing annotations file: {}".format(annotations_file))
    df = pd.read_csv(annotations_file)
    for e in df.to_dict(orient='records'):
        c.execute(insert, [ int(e['start']), int(e['end']), 
                            int(e['end']) - int(e['start']),
                            e['id'], e['type'], e['name'] ] 
                 )

# ---------------------------------------------------------------------------
# end of GENES table
# ---------------------------------------------------------------------------


# ---------------------------------------------------------------------------
# clean up
# ---------------------------------------------------------------------------
conn.commit()
conn.close()
